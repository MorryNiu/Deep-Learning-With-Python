{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two different way to deal with sequence\n",
    "1. recurrent neural network\n",
    "2. 1D Convnet\n",
    "\n",
    "Application including\n",
    "- doc classification, time sequence classification\n",
    "- compare how closely related are between two different doc or stock\n",
    "- language translation, sequence to sequence learning\n",
    "- Sentiment analysis\n",
    "- time sequence prediction, for instance, weather forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 unique tokens.\n",
      "{'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}\n"
     ]
    }
   ],
   "source": [
    "samples = ['The cat sat on the mat', 'The dog ate my homework.']\n",
    "\n",
    "tk = Tokenizer(num_words=1000)\n",
    "tk.fit_on_texts(samples)\n",
    "\n",
    "sequences = tk.texts_to_sequences(samples)\n",
    "\n",
    "one_hot_results = tk.texts_to_matrix(samples, mode='binary')\n",
    "\n",
    "word_index = tk.word_index\n",
    "print('Found {} unique tokens.'.format(len(word_index)))\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## instantiate a Embedding layer\n",
    "https://zhuanlan.zhihu.com/p/27830489 explaination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "max_features = 10000\n",
    "maxlen = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(10000, 8, input_length=maxlen))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 4s 187us/step - loss: 0.6759 - acc: 0.6043 - val_loss: 0.6398 - val_acc: 0.6812\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.5657 - acc: 0.7429 - val_loss: 0.5467 - val_acc: 0.7204\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 3s 150us/step - loss: 0.4752 - acc: 0.7809 - val_loss: 0.5113 - val_acc: 0.7386\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.4263 - acc: 0.8078 - val_loss: 0.5008 - val_acc: 0.7454\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 3s 152us/step - loss: 0.3930 - acc: 0.8256 - val_loss: 0.4981 - val_acc: 0.7540\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.3668 - acc: 0.8396 - val_loss: 0.5014 - val_acc: 0.7528\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.3435 - acc: 0.8534 - val_loss: 0.5052 - val_acc: 0.7520\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.3223 - acc: 0.8656 - val_loss: 0.5132 - val_acc: 0.7486\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 3s 153us/step - loss: 0.3022 - acc: 0.8764 - val_loss: 0.5214 - val_acc: 0.7490\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 3s 151us/step - loss: 0.2839 - acc: 0.8860 - val_loss: 0.5303 - val_acc: 0.7470\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pre-trained embedding\n",
    "like word2vec or GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-01-04 15:55:30--  http://mng.bz/0tIo\n",
      "正在解析主机 mng.bz (mng.bz)... 35.166.24.88\n",
      "正在连接 mng.bz (mng.bz)|35.166.24.88|:80... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 301 Moved Permanently\n",
      "位置：https://mng.bz/0tIo [跟随至新的 URL]\n",
      "--2019-01-04 15:55:31--  https://mng.bz/0tIo\n",
      "正在连接 mng.bz (mng.bz)|35.166.24.88|:443... 已连接。\n",
      "警告: 无法验证 mng.bz 的由 “CN=Go Daddy Secure Certificate Authority - G2,OU=http://certs.godaddy.com/repository/,O=GoDaddy.com\\\\, Inc.,L=Scottsdale,ST=Arizona,C=US” 颁发的证书:\n",
      "  无法本地校验颁发者的权限。\n",
      "已发出 HTTP 请求，正在等待回应... 301 \n",
      "位置：http://s3.amazonaws.com/text-datasets/aclImdb.zip [跟随至新的 URL]\n",
      "--2019-01-04 15:55:31--  http://s3.amazonaws.com/text-datasets/aclImdb.zip\n",
      "正在解析主机 s3.amazonaws.com (s3.amazonaws.com)... 52.216.239.45\n",
      "正在连接 s3.amazonaws.com (s3.amazonaws.com)|52.216.239.45|:80... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 302 Found\n",
      "位置：http://120.52.51.16/s3.amazonaws.com/text-datasets/aclImdb.zip [跟随至新的 URL]\n",
      "--2019-01-04 15:55:32--  http://120.52.51.16/s3.amazonaws.com/text-datasets/aclImdb.zip\n",
      "正在连接 120.52.51.16:80... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：60711700 (58M) [application/zip]\n",
      "正在保存至: “fullIMDB.zip”\n",
      "\n",
      "fullIMDB.zip        100%[===================>]  57.90M  2.83MB/s  用时 51s       \n",
      "\n",
      "2019-01-04 15:56:23 (1.14 MB/s) - 已保存 “fullIMDB.zip” [60711700/60711700])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download the dataset\n",
    "!wget --no-check-certificate http://mng.bz/0tIo -O fullIMDB.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "train\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "subprocess.call(['unzip', 'fullIMDB.zip'])\n",
    "\n",
    "res = subprocess.check_output([\"ls\", \"aclImdb/\"])\n",
    "\n",
    "for line in res.splitlines():\n",
    "    print(line.decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = subprocess.check_output([\"bash\", \"-c\", \"echo $PWD\"])\n",
    "root = root.splitlines()[0].decode('UTF-8')\n",
    "imdb_dir = root + \"/aclImdb\"\n",
    "\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fn in os.listdir(dir_name):\n",
    "        if fn[-4:] == '.txt':\n",
    "            with open(os.path.join(dir_name, fn)) as f:\n",
    "                texts.append(f.read())\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000\n"
     ]
    }
   ],
   "source": [
    "print(len(labels), len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88582 unique tokens.\n",
      "Shape of data tensor: (25000, 100)\n",
      "Shape of label tensor: (25000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 100\n",
    "training_samples = 200\n",
    "validation_samples = 10000\n",
    "max_words = 10000\n",
    "\n",
    "tk = Tokenizer(num_words=max_words)\n",
    "tk.fit_on_texts(texts)\n",
    "sequences = tk.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tk.word_index\n",
    "print('Found {} unique tokens.'.format(len(word_index)))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# generate [0,1,2,...,data.shape[0]]\n",
    "indices = np.arange(data.shape[0])\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples:training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-01-04 16:37:09--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "正在解析主机 nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "正在连接 nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 302 Found\n",
      "位置：https://nlp.stanford.edu/data/glove.6B.zip [跟随至新的 URL]\n",
      "--2019-01-04 16:37:10--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "正在连接 nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：862182613 (822M) [application/zip]\n",
      "正在保存至: “glove.6B.zip”\n",
      "\n",
      "glove.6B.zip         65%[============>       ] 538.54M   212KB/s  剩余 16m 52s   "
     ]
    }
   ],
   "source": [
    "if(\"glove.6B.zip\" not in os.listdir(root)):\n",
    "    # download the dataset\n",
    "    !wget --no-check-certificate http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
