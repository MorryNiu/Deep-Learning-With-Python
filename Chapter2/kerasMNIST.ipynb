{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# NN architecture\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# set up labels\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# helper fucntion\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train image dataset\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label dataset\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape = (28 * 28,)))\n",
    "network.add(layers.Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28*28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the process\n",
    "network.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare these categorical data\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 15s 253us/step - loss: 0.0395 - acc: 0.9922\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 15s 257us/step - loss: 0.0323 - acc: 0.9939\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 15s 254us/step - loss: 0.0279 - acc: 0.9951\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 15s 257us/step - loss: 0.0257 - acc: 0.9958\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 15s 255us/step - loss: 0.0237 - acc: 0.9963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb34d129b0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_images, train_labels, epochs = 5, batch_size=32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch size and epoch\n",
    "\n",
    "```python\n",
    "batch = train_images[:128]\n",
    "```\n",
    "This trainning process call SGD short for stochastic gradient descent. If we chose the batch number similar as the sample size number, it is essentially stochastic gradient descent. If the batch number equals 1, then it called **real** SGD. If the batch number was inbetween, we call it **mini-batch** SGD.\n",
    "\n",
    "Batch size is the number of training instance let seen by the model during each training iteration.\n",
    "\n",
    "Since we cannot process all the data at once, so we need to divide our dataset into small pieces and give it one by one to our computer and update the weights of our neural network at the end of every step.\n",
    "\n",
    "### why we use more than one epoch?\n",
    "It offer the NN more chance to go through the data, therefore, more number of times the weight are changed in NN, and the curve goes from underfitting to optimal to overfitting.\n",
    "\n",
    "### so what is a good number for epoch?\n",
    "It depends on how diverse is your data is. Technically, more diverse, larger epoch number.\n",
    "\n",
    "### how batch size affect accuracy? (see Question 2 in detail)\n",
    "For now, empirically we just know that large batch result in quicker trainning but normally lower accuracy.  \n",
    "\n",
    "### how do we update a batch of data at once?\n",
    "it doesn't matter that much whether you give 100 or 256 or 2048 or 10000 (batch size) images as long as it fits in the memory of your (GPU) hardware.\n",
    "\n",
    "\n",
    "### Question here\n",
    "1. How does batch of data been update at once?\n",
    "\n",
    "2. Empirically, there is not much difference between large batch number and small batch number when compare to their result in this example. However, with larger batch number, the trainning speed has significantly boosted. So what is the really difference between them?\n",
    "\n",
    "### Answer\n",
    "1. you just sum up all values of GD, and then apply them to weights at once\n",
    "\n",
    "2. From Andrew NG cs229-note, for single update, it also known as LMS (least mean square) update. $\\theta_j := \\theta_j + \\alpha\\Sigma(y^i-h_\\theta(x^i))x_j^i$ As this method looks at every example in the entire training set on every step, so it called **batch gradient desent**. $\\theta_j := \\theta_j + \\alpha(y^i-h_\\theta(x^i))x_j^i$ (for each i) For this one, we update the parameter based on one piece of data only so it call **stochastic gradient desent**. Comparing to each update, batch update takes longer than SGD, however compare to the whole dataset SGD takes much more times to finish. ***In batch gradient descent***, you compute the gradient over the entire dataset, averaging over potentially a vast amount of information. It takes lots of memory to do that. But the real handicap is the batch gradient trajectory land you in a bad spot (saddle point). ***In pure SGD***, on the other hand, you update your parameters by adding (minus sign) the gradient computed on a single instance of the dataset. Since it's based on one random data point, it's very noisy and may go off in a direction far from the batch gradient. However, the noisiness is exactly what you want in non-convex optimization, because it helps you escape from saddle points or local minima. The disadvantage is it's terribly inefficient and you need to loop over the entire dataset many times to find a good solution. ***The minibatch methodology*** is a compromise that injects enough noise to each gradient update, while achieving a relative speedy convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 37us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13549849958815474, 0.9787]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test our model\n",
    "network.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise a sample image, the method that writen on the book does not work here\n",
    "\n",
    "#digit = train_images[0]\n",
    "#plt.imshow(digit.reshape(28,28), cmap='Greys', interpolation='None')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = train_images[:128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 784)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
